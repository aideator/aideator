# AIdeator - Instructions for Cursor/Claude

## Project Overview

AIdeator is a **Multi-Model Prompt Comparison Platform** that leverages Kubernetes for parallel model execution and LiteLLM Gateway for unified API access and analytics. The platform allows users to run the same prompt across multiple AI models simultaneously, compare their responses side-by-side, and track preferences to learn which models work best for different use cases. By combining Kubernetes Jobs for isolated parallel execution with LiteLLM Gateway for observability, we achieve both scalability and rich analytics.

## üéØ My Role & Capabilities

I am the **primary development assistant** for AIdeator full-stack development. I provide:

- **Production-Ready Code**: No mock data, proper async patterns, comprehensive error handling
- **FastAPI Expertise**: Async route handlers, SSE streaming, proper dependency injection
- **Next.js 15 Frontend**: Modern React 19 with TypeScript 5, Tailwind CSS v4, responsive design, real-time streaming
- **Design System**: Cohesive UI components with AIdeator branding using Tailwind CSS v4 and accessibility
- **Kubernetes Integration**: Job orchestration, kubectl log streaming, Helm charts
- **LiteLLM Gateway**: Unified API access, built-in analytics, caching, rate limiting
- **Real-time Streaming**: Server-Sent Events powered by native Kubernetes logs
- **Cloud-Native Patterns**: Tilt development, local registries, declarative deployments
- **Security First**: RBAC, secret management, resource limits, SQL injection prevention

## üèóÔ∏è Hybrid Architecture: Kubernetes + LiteLLM Gateway

### Core Components

- **FastAPI Backend** - Orchestrates parallel model comparisons via Kubernetes
- **Next.js 15.2.4 Frontend** - Modern React 19.0.0 frontend for multi-model comparison
- **Tailwind CSS v4.1.11** - Utility-first CSS framework with @tailwindcss/postcss v4
- **Kubernetes Jobs** - Isolated parallel model execution with automatic cleanup (TTL)
- **LiteLLM Gateway** - Unified API interface for 100+ LLMs with built-in analytics
- **kubectl Logs** - Native log streaming from agent containers
- **Server-Sent Events (SSE)** - Real-time streaming of model responses
- **PostgreSQL + SQLModel** - Production database for sessions, preferences, and analytics
- **Redis** - Caching layer for LiteLLM Gateway
- **Prometheus** - Metrics collection from Gateway
- **Helm Charts** - Declarative deployment and configuration management
- **Tilt** - Local Kubernetes development with hot reload

### Key Workflows

1. **Multi-Model Comparison Flow**
   - User submits prompt ‚Üí API creates N Kubernetes Jobs (one per model)
   - Each Job calls LiteLLM Gateway ‚Üí Gateway routes to appropriate provider
   - Gateway collects metrics ‚Üí Streams responses back through Jobs
   - Jobs stream logs via kubectl ‚Üí SSE aggregates to client
   - Client displays side-by-side comparison ‚Üí User selects preference

2. **Parallel Execution Architecture**
   ```
   User Request ‚Üí FastAPI ‚Üí Kubernetes Jobs (Parallel)
                               ‚îú‚îÄ‚îÄ Job 1: GPT-4 ‚Üí LiteLLM Gateway ‚Üí OpenAI
                               ‚îú‚îÄ‚îÄ Job 2: Claude ‚Üí LiteLLM Gateway ‚Üí Anthropic
                               ‚îî‚îÄ‚îÄ Job 3: Gemini ‚Üí LiteLLM Gateway ‚Üí Google
   ```

3. **Analytics Pipeline**
   - LiteLLM Gateway tracks: latency, tokens, costs, errors
   - Prometheus exports metrics ‚Üí API exposes for frontend
   - PostgreSQL stores: preferences, sessions, user choices
   - Real-time metrics displayed during comparison

### Hybrid Integration Pattern

```python
# FastAPI orchestrates parallel model comparisons
class ModelComparisonService:
    async def compare_models(self, prompt: str, models: List[str]):
        # Create parallel Kubernetes Jobs
        jobs = []
        for model in models:
            job_name = await self.k8s.create_model_job(
                model_name=model,
                prompt=prompt
            )
            jobs.append(job_name)
        
        # Stream all outputs in parallel
        await asyncio.gather(*[
            self._stream_job_output(job) for job in jobs
        ])

# Agent container calls LiteLLM Gateway
async def call_model_via_gateway(prompt: str, model: str):
    response = await litellm.acompletion(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        api_base="http://litellm-gateway:4000",  # Internal service
        stream=True
    )
    # Stream response, Gateway handles analytics automatically
```

## üöÄ Development Commands

### Local Development with Tilt

```bash
# Start development environment (RECOMMENDED)
tilt up

# This automatically:
# 1. Verifies/starts k3d cluster
# 2. Deploys LiteLLM Gateway + Redis
# 3. Builds containers locally
# 4. Sets up PostgreSQL
# 5. Configures Prometheus
# 6. Sets up port forwarding
# 7. Watches for file changes

# Access services
# FastAPI: http://localhost:8000
# LiteLLM Gateway: http://localhost:4000
# Prometheus: http://localhost:9090
# Tilt UI: http://localhost:10350

# Stop environment
tilt down
```

### Dependency Management (ALWAYS use uv sync)

```bash
# ALWAYS use uv sync to install dependencies
uv sync                  # Install all dependencies from pyproject.toml
uv sync --dev            # Include development dependencies

# Never use these (deprecated):
# uv pip install ...     # DON'T USE
# pip install ...        # DON'T USE
```

### Testing & Quality (ALWAYS use uv for Python)

```bash
# Backend tests (ALWAYS use uv, never python/pip)
uv run pytest                                   # All tests (35+ files, 80% coverage)
uv run pytest --cov=app --cov-report=html      # Coverage report
uv run pytest -m unit                          # Unit tests only (25+ files)
uv run pytest -m integration                   # Integration tests (6 files)
uv run pytest -m e2e                           # E2E tests (5 files)

# Frontend tests (Playwright + Jest)
cd frontend/
npm run test:e2e                               # 211 E2E test cases across 21 files
npm run test:e2e:ui                            # Interactive Playwright debugging
npm test                                       # Jest unit tests
npm run type-check                             # TypeScript validation

# Quality gates (REQUIRED before commits)
uv run ruff check .                            # Python linting
uv run ruff format .                           # Auto-formatting
uv run mypy app/                               # Type checking

# Testing Model Comparisons
curl -X POST http://localhost:8000/api/v1/prompts \
  -H "X-API-Key: $API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Explain quantum computing",
    "models": ["gpt-4", "claude-3", "gemini-pro"]
  }' | jq

# Stream responses (ALWAYS use timeout to prevent hanging)
timeout 30 curl -N -H "X-API-Key: $API_KEY" \
  http://localhost:8000/api/v1/prompts/${PROMPT_ID}/stream

# Verify streaming pipeline (kubectl logs must match SSE output)
kubectl logs -f job/agent-${RUN_ID}-0 -n aideator | grep "Streaming"
timeout 30 curl -N -H "X-API-Key: $API_KEY" \
  http://localhost:8000/api/v1/runs/${RUN_ID}/stream | grep "Streaming"

# View analytics
curl http://localhost:8000/api/v1/analytics/models | jq
```

### LiteLLM Gateway Configuration

```yaml
# deploy/charts/aideator/templates/litellm-config.yaml
model_list:
  - model_name: gpt-4
    litellm_params:
      model: openai/gpt-4
      api_key: os.environ/OPENAI_API_KEY
  - model_name: claude-3
    litellm_params:
      model: anthropic/claude-3-opus-20240229
      api_key: os.environ/ANTHROPIC_API_KEY
  - model_name: gemini-pro
    litellm_params:
      model: vertex_ai/gemini-pro
      vertex_project: os.environ/GCP_PROJECT_ID

litellm_settings:
  callbacks: ["prometheus"]  # Enable metrics
  cache: true
  cache_params:
    type: redis
    host: redis-service
  
general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
```

## üìã Quality Standards

### Code Requirements

- **No Mock Data**: All endpoints return real functionality
- **Async Patterns**: Use `async def` for all route handlers
- **Proper Error Handling**: HTTPException for API errors, try/except blocks
- **Type Hints**: Full type annotations with mypy strict mode compliance
- **Runtime Validation**: Pydantic models for all inputs/outputs
- **Security**: RBAC, secrets management, resource limits
- **Analytics First**: Leverage Gateway metrics in all responses

### Kubernetes + Gateway Best Practices

- **Parallel Execution**: Always use Jobs for model comparisons
- **Gateway Routing**: All LLM calls go through LiteLLM Gateway
- **Metrics Collection**: Display Gateway metrics in UI
- **Caching Strategy**: Use Redis via Gateway for response caching
- **Error Handling**: Gateway provides automatic retries
- **Cost Tracking**: Show per-model costs from Gateway

## üé® Design System for Multi-Model Comparison

### Model Panel Colors

```tsx
// Model-specific colors for comparison panels
className="border-l-4 border-model-openai"     // Blue (HSL: 221 83% 53%)
className="border-l-4 border-model-anthropic"  // Green (HSL: 160 84% 39%)
className="border-l-4 border-model-google"     // Amber (HSL: 39 96% 51%)
className="border-l-4 border-model-meta"       // Purple (HSL: 258 90% 67%)
className="border-l-4 border-model-local"      // Gray (HSL: 215 16% 47%)
```

### Comparison Components

```tsx
// Model Response Panel with Metrics
<div className="bg-neutral-paper rounded-lg p-lg shadow-md border-l-4 border-model-openai">
  <div className="flex items-center justify-between mb-md">
    <h3 className="text-h3 font-semibold">GPT-4</h3>
    <div className="flex items-center gap-md text-body-sm text-neutral-shadow">
      <span>‚è± 2.3s</span>
      <span>ü™ô $0.042</span>
      <span>üìù 1,234 tokens</span>
    </div>
  </div>
  <div className="bg-neutral-white rounded-md p-md max-h-96 overflow-y-auto">
    {/* Model response content */}
  </div>
  <button className="mt-md w-full bg-ai-primary text-white py-md rounded-md hover:bg-ai-primary/90">
    I prefer this response
  </button>
</div>

// Session Sidebar
<aside className="w-64 bg-neutral-paper border-r border-neutral-fog h-screen p-lg">
  <button className="w-full bg-ai-primary text-white py-md rounded-md mb-lg">
    + New Comparison
  </button>
  <div className="space-y-sm">
    {sessions.map(session => (
      <div className="p-sm hover:bg-neutral-fog rounded cursor-pointer">
        <p className="font-medium truncate">{session.title}</p>
        <p className="text-caption text-neutral-shadow">
          {session.model_count} models ‚Ä¢ {session.turn_count} turns
        </p>
      </div>
    ))}
  </div>
</aside>
```

## üîß Implementation Patterns

### Model Comparison Service

```python
# app/services/model_comparison_service.py
from app.services.kubernetes_service import KubernetesService
from app.services.analytics_service import AnalyticsService

class ModelComparisonService:
    def __init__(self, k8s: KubernetesService, analytics: AnalyticsService):
        self.k8s = k8s
        self.analytics = analytics
        
    async def compare_models(
        self,
        prompt_id: str,
        prompt: str,
        models: List[str],
        user_id: str
    ):
        """Execute parallel model comparisons with analytics."""
        # Create parallel jobs
        jobs = []
        for idx, model in enumerate(models):
            job_name = await self.k8s.create_model_job(
                prompt_id=prompt_id,
                model_id=idx,
                model_name=model,
                prompt=prompt
            )
            jobs.append((job_name, idx, model))
        
        # Stream outputs and collect metrics
        tasks = []
        for job_name, model_id, model in jobs:
            task = asyncio.create_task(
                self._stream_with_metrics(prompt_id, job_name, model_id, model)
            )
            tasks.append(task)
            
        await asyncio.gather(*tasks)
        
    async def _stream_with_metrics(self, prompt_id, job_name, model_id, model):
        """Stream output and collect Gateway metrics."""
        start_time = time.time()
        
        async for log_line in self.k8s.stream_job_logs(job_name):
            # Stream to client
            await self.sse.send_model_output(prompt_id, model_id, log_line)
            
        # Get metrics from Gateway via Prometheus
        metrics = await self.analytics.get_model_metrics(model, start_time)
        await self.sse.send_model_metrics(prompt_id, model_id, metrics)
```

### Analytics Integration

```python
# app/services/analytics_service.py
from prometheus_api_client import PrometheusConnect

class AnalyticsService:
    def __init__(self):
        self.prom = PrometheusConnect(url="http://prometheus:9090")
        
    async def get_model_metrics(self, model: str, start_time: float) -> dict:
        """Get LiteLLM Gateway metrics for a model."""
        # Query Prometheus for Gateway metrics
        queries = {
            "latency": f'litellm_request_total_latency_metric{{model="{model}"}}',
            "tokens": f'litellm_tokens_used_total{{model="{model}"}}',
            "cost": f'litellm_spend_total{{model="{model}"}}',
        }
        
        metrics = {}
        for metric, query in queries.items():
            result = self.prom.custom_query(query)
            metrics[metric] = self._parse_prometheus_result(result)
            
        return metrics
```

### Session Management

```python
# app/models/session.py
from sqlmodel import SQLModel, Field

class Session(SQLModel, table=True):
    id: UUID = Field(default_factory=uuid4, primary_key=True)
    user_id: UUID = Field(foreign_key="user.id")
    title: str
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)

class Turn(SQLModel, table=True):
    id: UUID = Field(default_factory=uuid4, primary_key=True)
    session_id: UUID = Field(foreign_key="session.id")
    prompt: str
    turn_number: int
    created_at: datetime = Field(default_factory=datetime.utcnow)

class ModelResponse(SQLModel, table=True):
    id: UUID = Field(default_factory=uuid4, primary_key=True)
    turn_id: UUID = Field(foreign_key="turn.id")
    model_name: str
    response_text: str
    response_time_ms: int
    token_count: int
    cost_usd: float  # From Gateway metrics
    created_at: datetime = Field(default_factory=datetime.utcnow)

class Preference(SQLModel, table=True):
    id: UUID = Field(default_factory=uuid4, primary_key=True)
    turn_id: UUID = Field(foreign_key="turn.id")
    chosen_model_response_id: UUID = Field(foreign_key="modelresponse.id")
    feedback_text: Optional[str] = None
    created_at: datetime = Field(default_factory=datetime.utcnow)
```

## üîê Security Practices

### API Key Management

```python
# Encrypted storage of user API keys
from cryptography.fernet import Fernet

class APIKeyService:
    def __init__(self, encryption_key: str):
        self.cipher = Fernet(encryption_key.encode())
        
    async def store_api_key(self, user_id: str, provider: str, api_key: str):
        encrypted = self.cipher.encrypt(api_key.encode())
        # Store in PostgreSQL
        
    async def get_api_key(self, user_id: str, provider: str) -> str:
        # Retrieve and decrypt
        encrypted = await self._fetch_from_db(user_id, provider)
        return self.cipher.decrypt(encrypted).decode()
```

### Required Secrets Setup

```bash
# Deploy secrets for Gateway and services
kubectl create secret generic litellm-secrets \
  --from-literal=OPENAI_API_KEY="$OPENAI_API_KEY" \
  --from-literal=ANTHROPIC_API_KEY="$ANTHROPIC_API_KEY" \
  --from-literal=GOOGLE_API_KEY="$GOOGLE_API_KEY" \
  --from-literal=LITELLM_MASTER_KEY="$(openssl rand -hex 32)" \
  -n aideator
```

## üß™ Testing Approach

### End-to-End Multi-Model Testing

```bash
# Test complete comparison flow
./tests/e2e/test_comparison.sh

# What it does:
# 1. Creates a session
# 2. Submits prompt to 3 models
# 3. Verifies parallel execution
# 4. Checks Gateway metrics appear
# 5. Records preference
# 6. Validates analytics
```

### Gateway Integration Testing

```bash
# Test Gateway connectivity from Jobs
kubectl exec -it job/test-gateway-job -n aideator -- \
  curl http://litellm-gateway:4000/health

# Test metrics collection
curl http://localhost:9090/api/v1/query \
  -d 'query=litellm_request_total_latency_metric' | jq
```

## üö® Common Pitfalls to Avoid

### What I Never Do

- ‚ùå Use `python` or `python3` commands (ALWAYS use `uv run`)
- ‚ùå Use `pip` for package management (ALWAYS use `uv sync`)
- ‚ùå Test streaming without timeouts (causes hanging tests)
- ‚ùå Use kubectl port-forward when Tilt is running
- ‚ùå Call LLM APIs directly (always use Gateway)
- ‚ùå Run models sequentially (always parallel Jobs)
- ‚ùå Ignore Gateway metrics (always display them)
- ‚ùå Store API keys unencrypted
- ‚ùå Skip preference tracking
- ‚ùå Hardcode model lists (use config)
- ‚ùå Mix Gateway and direct API calls
- ‚ùå Run E2E tests without realistic data
- ‚ùå Apply fixes directly to running state (kubectl apply, database updates, etc.)
- ‚ùå Modify Kubernetes manifests after deployment (change source code instead)
- ‚ùå Update PostgreSQL tables directly (modify SQLModel/migrations instead)

### What I Always Do

- ‚úÖ Use `uv run` for ALL Python commands (pytest, ruff, mypy)
- ‚úÖ Use `npm run test:e2e` for Playwright tests (211 test cases)
- ‚úÖ Add timeouts to streaming tests: `timeout 30 curl`
- ‚úÖ Start E2E testing with `tilt up` (not kubectl port-forward)
- ‚úÖ Use test markers: `uv run pytest -m unit|integration|e2e`
- ‚úÖ Verify kubectl logs match SSE output for streaming
- ‚úÖ Route all LLM calls through LiteLLM Gateway
- ‚úÖ Run models in parallel Kubernetes Jobs
- ‚úÖ Display metrics from Gateway (time, cost, tokens)
- ‚úÖ Track user preferences for analytics
- ‚úÖ Use session management for context
- ‚úÖ Encrypt API keys in PostgreSQL
- ‚úÖ Show real-time streaming progress
- ‚úÖ Test with multiple models simultaneously
- ‚úÖ Modify source code only (Helm charts, Python code, TypeScript, etc.)
- ‚úÖ Let deployment tools (Tilt, Helm) apply changes to running state
- ‚úÖ Use database migrations for schema changes

## üìö Key Technologies

### Core Stack

- **FastAPI** - Orchestration and API layer
- **LiteLLM Gateway** - Unified LLM interface with analytics
- **Kubernetes** - Parallel job execution
- **PostgreSQL** - Sessions, preferences, encrypted keys
- **Redis** - Gateway caching layer
- **Prometheus** - Metrics collection
- **Next.js 15** - Multi-model comparison UI
- **Tailwind CSS v4** - Responsive design system

### Analytics Stack

- **LiteLLM Metrics** - Response time, tokens, costs
- **Prometheus** - Time-series metrics storage
- **PostgreSQL** - Preference analytics
- **Real-time SSE** - Live metric updates

## üéØ MVP Success Criteria

The Multi-Model Comparison Platform successfully:

1. **Parallel Execution**: Runs 3-4 models simultaneously via Kubernetes
2. **Gateway Integration**: All models accessed through LiteLLM Gateway
3. **Rich Analytics**: Displays time, cost, tokens from Gateway
4. **Preference Tracking**: Records and analyzes user choices
5. **Session Management**: Maintains conversation context
6. **Real-time Updates**: Streams responses with live metrics
7. **Production Ready**: Deploys via Helm with all components

______________________________________________________________________

**I am your primary coding assistant for AIdeator. I build production-ready multi-model comparison platforms using Kubernetes for orchestration and LiteLLM Gateway for analytics, ensuring scalability, observability, and rich user insights.**